{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe31be9-9dc0-4fb3-854f-c3c7d4b9b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jaxtyping\n",
    "!pip install git+https://github.com/Deep-Learning-Profiling-Tools/triton-viz@v1\n",
    "!export LC_ALL=\"en_US.UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13bb848a-a7e2-4e66-90f1-f13e7ff35570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't run these commands, they only install triton basically, but we already have a compatible version of triton installed\n",
    "# !curl --output \"triton-3.0.0-cp310-cp310-linux_x86_64.whl\" \"https://dl.cloudsmith.io/public/test-wha/triton-puzzles/raw/files/triton-3.0.0-cp310-cp310-linux_x86_64.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e07977d2-14df-49d1-b4ea-ffcb9cf56217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: triton\n",
      "Version: 2.3.0\n",
      "Summary: A language and compiler for custom Deep Learning operations\n",
      "Home-page: https://github.com/openai/triton/\n",
      "Author: Philippe Tillet\n",
      "Author-email: phil@openai.com\n",
      "License: \n",
      "Location: /usr/lib64/python3.12/site-packages\n",
      "Requires: filelock\n",
      "Required-by: triton-viz\n"
     ]
    }
   ],
   "source": [
    "!pip show triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306721ec-8fc7-4ac7-8e78-2fc1bd64a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load rocm/gfx9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c378bc-ee91-477f-8a03-7878f6f4823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "from torch import Tensor\n",
    "import triton.language as tl\n",
    "import jaxtyping\n",
    "from jaxtyping import Float32, Int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b191a4a-3200-4dc6-ab28-dfe4c550ba74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13dfa468-0208-48cd-afc3-72652e79ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def add_kernel(x_ptr,  # *Pointer* to first input vector.\n",
    "               y_ptr,  # *Pointer* to second input vector.\n",
    "               output_ptr,  # *Pointer* to output vector.\n",
    "               n_elements,  # Size of the vector.\n",
    "               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
    "               # NOTE: `constexpr` so it can be used as a shape value.\n",
    "               ):\n",
    "    # There are multiple 'programs' processing different data. We identify which program\n",
    "    # we are here:\n",
    "    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.\n",
    "    # This program will process inputs that are offset from the initial data.\n",
    "    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
    "    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
    "    # Note that offsets is a list of pointers:\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
    "    mask = offsets < n_elements\n",
    "    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n",
    "    # multiple of the block size.\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    output = x + y\n",
    "    # Write x + y back to DRAM.\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc6aa690-2c2a-4c8e-9d49-3ca75227f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    # We need to preallocate the output.\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_cuda and y.is_cuda and output.is_cuda\n",
    "    n_elements = output.numel()\n",
    "    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n",
    "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
    "    # In this case, we use a 1D grid where the size is the number of blocks:\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "    # NOTE:\n",
    "    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n",
    "    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n",
    "    #  - Don't forget to pass meta-parameters as keywords arguments.\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n",
    "    # running asynchronously at this point.\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c140800-3ecd-4fa9-b6c5-d9fef4594c37",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "<built-in function load_binary> returned NULL without setting an exception",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(size, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m output_torch \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m y\n\u001b[0;32m----> 6\u001b[0m output_triton \u001b[38;5;241m=\u001b[39m \u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_torch)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_triton)\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36madd\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m meta: (triton\u001b[38;5;241m.\u001b[39mcdiv(n_elements, meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOCK_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m]), )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# NOTE:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#  - Each torch.tensor object is implicitly converted into a pointer to its first element.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#  - Don't forget to pass meta-parameters as keywords arguments.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43madd_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_elements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLOCK_SIZE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# running asynchronously at this point.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m<string>:78\u001b[0m, in \u001b[0;36madd_kernel\u001b[0;34m(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE, grid, num_warps, num_ctas, num_stages, waves_per_eu, matrix_instr_nonkdim, enable_warp_specialization, extern_libs, stream, warmup, device, device_type)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib64/python3.12/site-packages/triton/compiler/compiler.py:698\u001b[0m, in \u001b[0;36mCompiledKernel.__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_wrapper\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(name)\n",
      "File \u001b[0;32m/usr/lib64/python3.12/site-packages/triton/compiler/compiler.py:689\u001b[0m, in \u001b[0;36mCompiledKernel._init_handles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared \u001b[38;5;241m>\u001b[39m max_shared:\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutOfResources(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared, max_shared, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared memory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 689\u001b[0m mod, func, n_regs, n_spills \u001b[38;5;241m=\u001b[39m \u001b[43mfn_load_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbin_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_spills \u001b[38;5;241m=\u001b[39m n_spills\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_regs \u001b[38;5;241m=\u001b[39m n_regs\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function load_binary> returned NULL without setting an exception"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "size = 98432\n",
    "x = torch.rand(size, device='cuda')\n",
    "y = torch.rand(size, device='cuda')\n",
    "output_torch = x + y\n",
    "output_triton = add(x, y)\n",
    "print(output_torch)\n",
    "print(output_triton)\n",
    "print(f'The maximum difference between torch and triton is '\n",
    "      f'{torch.max(torch.abs(output_torch - output_triton))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e76c48-5ce7-4c23-b418-a4422f4bc22c",
   "metadata": {},
   "source": [
    "#### stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab9c9e9-7339-4ba4-bb7a-05e401268b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "<built-in function load_binary> returned NULL without setting an exception",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# no thread-block\u001b[39;00m\n\u001b[1;32m     31\u001b[0m output_torch \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m y\n\u001b[0;32m---> 32\u001b[0m \u001b[43madd\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_torch)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_triton)\n",
      "File \u001b[0;32m<string>:78\u001b[0m, in \u001b[0;36madd\u001b[0;34m(X, Y, Z, N, grid, num_warps, num_ctas, num_stages, waves_per_eu, matrix_instr_nonkdim, enable_warp_specialization, extern_libs, stream, warmup, device, device_type)\u001b[0m\n",
      "File \u001b[0;32m/usr/lib64/python3.12/site-packages/triton/compiler/compiler.py:698\u001b[0m, in \u001b[0;36mCompiledKernel.__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_wrapper\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 698\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(name)\n",
      "File \u001b[0;32m/usr/lib64/python3.12/site-packages/triton/compiler/compiler.py:689\u001b[0m, in \u001b[0;36mCompiledKernel._init_handles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared \u001b[38;5;241m>\u001b[39m max_shared:\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutOfResources(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared, max_shared, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared memory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 689\u001b[0m mod, func, n_regs, n_spills \u001b[38;5;241m=\u001b[39m \u001b[43mfn_load_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbin_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_spills \u001b[38;5;241m=\u001b[39m n_spills\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_regs \u001b[38;5;241m=\u001b[39m n_regs\n",
      "\u001b[0;31mSystemError\u001b[0m: <built-in function load_binary> returned NULL without setting an exception"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "BLOCK = 512\n",
    "\n",
    "# This is a GPU kernel in Triton.\n",
    "# Different instances of this\n",
    "# function may run in parallel.\n",
    "@triton.jit\n",
    "def add(X, Y, Z, N):\n",
    "   # In Triton, each kernel instance\n",
    "   # executes block operations on a\n",
    "   # single thread: there is no construct\n",
    "   # analogous to threadIdx\n",
    "   pid = tl.program_id(0)\n",
    "   # block of indices\n",
    "   idx = pid * BLOCK + tl.arange(0, BLOCK)\n",
    "   mask = idx < N\n",
    "   # Triton uses pointer arithmetics  \n",
    "   # rather than indexing operators\n",
    "   x = tl.load(X + idx, mask=mask)\n",
    "   y = tl.load(Y + idx, mask=mask)\n",
    "   tl.store(Z + idx, x + y, mask=mask)\n",
    "\n",
    "size = 100000\n",
    "x = torch.rand(size, device='cuda')\n",
    "y = torch.rand(size, device='cuda')\n",
    "z = torch.empty_like(x, device='cuda')\n",
    "\n",
    "grid = (math.ceil(size / BLOCK),)\n",
    "# no thread-block\n",
    "output_torch = x + y\n",
    "add[grid](x, y, z, x.shape[0])\n",
    "\n",
    "print(output_torch)\n",
    "print(output_triton)\n",
    "print(f'The maximum difference between torch and triton is '\n",
    "      f'{torch.max(torch.abs(output_torch - output_triton))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e8069-dcc4-45b4-8add-6ed9d8ccc719",
   "metadata": {},
   "source": [
    "#### stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab026b7-6e3e-4743-b287-af2f72ed506e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400]) tensor([ 14.2556,  10.9699,   4.5875, -19.7804, -42.4840], device='cuda:0')\n",
      "torch.Size([400]) tensor([ 14.2556,  10.9699,   4.5875, -19.7804, -42.4840], device='cuda:0')\n",
      "The maximum difference between torch and triton is 7.62939453125e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "from torch import Tensor\n",
    "import triton.language as tl\n",
    "import jaxtyping\n",
    "from jaxtyping import Float32, Int32\n",
    "\n",
    "# M rows, N cols\n",
    "@triton.jit\n",
    "def collapserows(Y, stride_ym, X, stride_xm, stride_xn, M, N):\n",
    "    # row index\n",
    "    m = tl.program_id(0)\n",
    "    # col indices\n",
    "    # this specific kernel only works for matrices that \n",
    "    # have less than BLOCK_SIZE columns\n",
    "    BLOCK_SIZE: tl.constexpr = 1024\n",
    "    n = tl.arange(0, BLOCK_SIZE)\n",
    "    # the memory address of all the elements\n",
    "    # that we want to load can be computed as follows\n",
    "    X = (X + m * stride_xm) + n * stride_xn\n",
    "    # load input data; pad out-of-bounds elements with 0 \n",
    "    x = tl.load(X, mask=n < N, other=0)\n",
    "    y = tl.sum(x)\n",
    "    # write back to Y\n",
    "    Y = Y + m * stride_ym\n",
    "    tl.store(Y, y)\n",
    "\n",
    "import torch\n",
    "# Allocate input/output tensors\n",
    "X = torch.normal(0, 1, size=(400, 400), device='cuda')\n",
    "Y = torch.zeros(X.shape[0], device='cuda')\n",
    "# SPMD launch grid\n",
    "grid = (X.shape[0], )\n",
    "# enqueue GPU kernel\n",
    "collapserows[grid](Y, Y.stride(0),\n",
    "              X, X.stride(0), X.stride(1),\n",
    "              X.shape[0]    , X.shape[1])\n",
    "\n",
    "Y_torch = torch.sum(X, dim=1)\n",
    "\n",
    "print(Y.shape, Y[:5])\n",
    "print(Y_torch.shape, Y_torch[:5])\n",
    "print(f'The maximum difference between torch and triton is '\n",
    "      f'{torch.max(torch.abs(Y_torch - Y))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0377be8-e148-42c5-9e02-b1ab23de8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0, 1, size=(8192, 8192), device='cuda')\n",
    "Y = torch.zeros(X.shape[0], device='cuda')\n",
    "grid = (X.shape[0], )\n",
    "# SPMD launch grid\n",
    "\n",
    "for i in range(100000):\n",
    "    Y_torch = torch.sum(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41303627-7a02-4099-9b6c-48552c1feb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0, 1, size=(8192, 8192), device='cuda')\n",
    "Y = torch.zeros(X.shape[0], device='cuda')\n",
    "grid = (X.shape[0], )\n",
    "# SPMD launch grid\n",
    "\n",
    "for i in range(100000):\n",
    "    collapserows[grid](Y, Y.stride(0),\n",
    "              X, X.stride(0), X.stride(1),\n",
    "              X.shape[0]    , X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d220a94-e86c-4c44-a4dc-2ba68552ac31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HIP_FORCE_DEV_KERNARG=0\n"
     ]
    }
   ],
   "source": [
    "%env HIP_FORCE_DEV_KERNARG=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e4de8-3ec1-443b-ba71-8932dd3802d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "from torch import Tensor\n",
    "import triton.language as tl\n",
    "import jaxtyping\n",
    "from jaxtyping import Float32, Int32\n",
    "\n",
    "@triton.jit\n",
    "def softmax(Y, stride_ym, stride_yn, X, stride_xm, stride_xn, M, N):\n",
    "    # row index\n",
    "    m = tl.program_id(0)\n",
    "    # col indices\n",
    "    # this specific kernel only works for matrices that \n",
    "    # have less than BLOCK_SIZE columns\n",
    "    BLOCK_SIZE: tl.constexpr = 1024\n",
    "    n = tl.arange(0, BLOCK_SIZE)\n",
    "    # the memory address of all the elements\n",
    "    # that we want to load can be computed as follows\n",
    "\n",
    "    \"\"\"\n",
    "    X = (X + m * stride_xm) + n * stride_xn\n",
    "    # load input data; pad out-of-bounds elements with 0 \n",
    "    neginf = -float('inf')\n",
    "    x = tl.load(X, mask=n < N, other=neginf)\n",
    "    # compute numerically-stable softmax\n",
    "    z = x - tl.max(x, axis=0)\n",
    "    #num = tl.exp(z)\n",
    "    #denom = tl.sum(num, axis=0)\n",
    "    #denom = tl.sum(x, axis=0)\n",
    "    #y = tl.exp(x)\n",
    "    # write back to Y\n",
    "    \"\"\"\n",
    "\n",
    "    # calculate e^(first elememnt)\n",
    "    value = tl.exp(tl.load(X + m * stride_xm + 0 * stride_xn))\n",
    "    \n",
    "    Y = Y + m * stride_ym + n * stride_yn\n",
    "    tl.store(Y, value, mask=n < N)\n",
    "    \n",
    "    \"\"\"\n",
    "    Y = Y + m * stride_ym + n * stride_yn\n",
    "    y = tl.sin(x)\n",
    "    #print(\"hi\")\n",
    "    #tl.printf(\"%d\", (y.shape[0]))\n",
    "    tl.store(Y, y, mask=n < N)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "import torch\n",
    "# Allocate input/output tensors\n",
    "# torch.zeros(512, 2, device='cuda')\n",
    "X = torch.zeros(512, 1, device='cuda')#torch.normal(0, 1, size=(512, 1), device='cuda')\n",
    "Y = torch.empty_like(X, device='cuda')\n",
    "\n",
    "print(X.dtype)\n",
    "\n",
    "# SPMD launch grid\n",
    "grid = (X.shape[0], )\n",
    "# enqueue GPU kernel\n",
    "softmax[grid](Y, Y.stride(0), Y.stride(1), \n",
    "              X, X.stride(0), X.stride(1),\n",
    "              X.shape[0]    , X.shape[1])\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586db3cd-9a57-4809-a12b-b0ba043e69d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MIOPEN_ENABLE_LOGGING=yes\n",
      "env: MIOPEN_ENABLE_LOGGING_CMD=yes\n",
      "env: MIOPEN_LOG_LEVEL=6\n"
     ]
    }
   ],
   "source": [
    "%env MIOPEN_ENABLE_LOGGING=yes\n",
    "%env MIOPEN_ENABLE_LOGGING_CMD=yes\n",
    "%env MIOPEN_LOG_LEVEL=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a21fef-807f-4d7c-99f0-c5f8002dee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "!echo $MIOPEN_ENABLE_LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ff8bc-4124-4b37-bce9-925eb4fe50ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31c08788-36a0-41d5-bc9c-1beb6cf5098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([0.1197, 0.6572, 0.7279,  ..., 3.1513, 1.7558, 0.4742], device='cuda:0')\n",
      "tensor([0.1264, 0.6572, 0.7279,  ..., 3.1514, 1.7558, 0.4742], device='cuda:0')\n",
      "The maximum difference between torch and triton is 9.727210998535156\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "from torch import Tensor\n",
    "import triton.language as tl\n",
    "import jaxtyping\n",
    "from jaxtyping import Float32, Int32\n",
    "\n",
    "@triton.jit\n",
    "def exp(Y, stride_yn, X, stride_xn, N):\n",
    "    # row index\n",
    "    n = tl.program_id(0)\n",
    "    # col indices\n",
    "    # this specific kernel only works for matrices that \n",
    "    # have less than BLOCK_SIZE columns\n",
    "    BLOCK_SIZE: tl.constexpr = 1024\n",
    "    n_block = tl.arange(0, BLOCK_SIZE)\n",
    "    # the memory address of all the elements\n",
    "    # that we want to load can be computed as follows\n",
    "\n",
    "    # calculate e^(first elememnt)\n",
    "    z = tl.load(X + n * stride_xn + n_block, mask=n_block < N)\n",
    "    \n",
    "    value = 1 + z + z * z / 2 + z * z * z / 6 + z * z * z * z / 24 + z * z * z * z * z / 120 + z * z * z * z * z * z / (120 * 6) + z * z * z * z * z * z * z / (120 * 6 * 7)\n",
    "\n",
    "    Y = Y + n * stride_yn + n_block\n",
    "    tl.store(Y, value, mask=n_block < N)\n",
    "    \n",
    "    \"\"\"\n",
    "    Y = Y + m * stride_ym + n * stride_yn\n",
    "    y = tl.sin(x)\n",
    "    #print(\"hi\")\n",
    "    #tl.printf(\"%d\", (y.shape[0]))\n",
    "    tl.store(Y, y, mask=n < N)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "import torch\n",
    "# Allocate input/output tensors\n",
    "# torch.zeros(512, 2, device='cuda')\n",
    "X = torch.normal(0, 1, size=(524288,), device='cuda')\n",
    "Y = torch.empty_like(X, device='cuda')\n",
    "\n",
    "print(X.dtype)\n",
    "\n",
    "# SPMD launch grid\n",
    "grid = (X.shape[0], )\n",
    "# enqueue GPU kernel\n",
    "exp[grid](Y, Y.stride(0), \n",
    "              X, X.stride(0),\n",
    "              X.shape[0])\n",
    "\n",
    "torch_out = torch.exp(X)\n",
    "\n",
    "print(Y)\n",
    "print(torch_out)\n",
    "\n",
    "print(f'The maximum difference between torch and triton is '\n",
    "      f'{torch.max(torch.abs(torch_out - Y))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456f35c-5bb9-4ba8-8eaa-a456baa39cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3a7b9e-834f-4fd6-bb70-affa6d6d73e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InterpretedFunction' from 'triton.interpreter' (/usr/lib64/python3.12/site-packages/triton/interpreter/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# @title Setup\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtriton_viz\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtriton_viz\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m record_builder\n",
      "File \u001b[0;32m~/triton_viz_install/triton-viz/triton_viz/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trace, dump, sample\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdraw\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collect_grid, draw_record\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m launch\n",
      "File \u001b[0;32m~/triton_viz_install/triton-viz/triton_viz/trace.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruntime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KernelInterface\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interpreter\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpretedFunction\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JITFunction\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m patch, record_builder\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'InterpretedFunction' from 'triton.interpreter' (/usr/lib64/python3.12/site-packages/triton/interpreter/__init__.py)"
     ]
    }
   ],
   "source": [
    "# @title Setup\n",
    "\n",
    "import triton_viz\n",
    "import inspect\n",
    "from triton_viz.interpreter import record_builder\n",
    "\n",
    "def test(puzzle, puzzle_spec, nelem={}, B={\"B0\": 32}, viz=True):\n",
    "    B = dict(B)\n",
    "    if \"N1\" in nelem and \"B1\" not in B:\n",
    "        B[\"B1\"] = 32\n",
    "    if \"N2\" in nelem and \"B2\" not in B:\n",
    "        B[\"B2\"] = 32\n",
    "\n",
    "    triton_viz.interpreter.record_builder.reset()\n",
    "    torch.manual_seed(0)\n",
    "    signature = inspect.signature(puzzle_spec)\n",
    "    args = {}\n",
    "    for n, p in signature.parameters.items():\n",
    "        print(p)\n",
    "        args[n + \"_ptr\"] = ([d.size for d in p.annotation.dims], p)\n",
    "    args[\"z_ptr\"] = ([d.size for d in signature.return_annotation.dims], None)\n",
    "\n",
    "    tt_args = []\n",
    "    for k, (v, t) in args.items():\n",
    "        tt_args.append(torch.rand(*v) - 0.5)\n",
    "        if t is not None and t.annotation.dtypes[0] == \"int32\":\n",
    "            tt_args[-1] = torch.randint(-100000, 100000, v)\n",
    "    grid = lambda meta: (triton.cdiv(nelem[\"N0\"], meta[\"B0\"]),\n",
    "                         triton.cdiv(nelem.get(\"N1\", 1), meta.get(\"B1\", 1)),\n",
    "                         triton.cdiv(nelem.get(\"N2\", 1), meta.get(\"B2\", 1)))\n",
    "\n",
    "    #for k, v in args.items():\n",
    "    #    print(k, v)\n",
    "    triton_viz.trace(puzzle)[grid](*tt_args, **B, **nelem)\n",
    "    z = tt_args[-1]\n",
    "    tt_args = tt_args[:-1]\n",
    "    z_ = puzzle_spec(*tt_args)\n",
    "    match = torch.allclose(z, z_, rtol=1e-3, atol=1e-3)\n",
    "    print(\"Results match:\",  match)\n",
    "    failures = False\n",
    "    if viz:\n",
    "        failures = triton_viz.launch()\n",
    "    if not match or failures:\n",
    "        print(\"Invalid Access:\", failures)\n",
    "        print(\"Yours:\", z)\n",
    "        print(\"Spec:\", z_)\n",
    "        print(torch.isclose(z, z_))\n",
    "        return\n",
    "    # PUPPIES!\n",
    "    from IPython.display import HTML\n",
    "    import random\n",
    "    print(\"Correct!\")\n",
    "    pups = [\n",
    "    \"2m78jPG\",\n",
    "    \"pn1e9TO\",\n",
    "    \"MQCIwzT\",\n",
    "    \"udLK6FS\",\n",
    "    \"ZNem5o3\",\n",
    "    \"DS2IZ6K\",\n",
    "    \"aydRUz8\",\n",
    "    \"MVUdQYK\",\n",
    "    \"kLvno0p\",\n",
    "    \"wScLiVz\",\n",
    "    \"Z0TII8i\",\n",
    "    \"F1SChho\",\n",
    "    \"9hRi2jN\",\n",
    "    \"lvzRF3W\",\n",
    "    \"fqHxOGI\",\n",
    "    \"1xeUYme\",\n",
    "    \"6tVqKyM\",\n",
    "    \"CCxZ6Wr\",\n",
    "    \"lMW0OPQ\",\n",
    "    \"wHVpHVG\",\n",
    "    \"Wj2PGRl\",\n",
    "    \"HlaTE8H\",\n",
    "    \"k5jALH0\",\n",
    "    \"3V37Hqr\",\n",
    "    \"Eq2uMTA\",\n",
    "    \"Vy9JShx\",\n",
    "    \"g9I2ZmK\",\n",
    "    \"Nu4RH7f\",\n",
    "    \"sWp0Dqd\",\n",
    "    \"bRKfspn\",\n",
    "    \"qawCMl5\",\n",
    "    \"2F6j2B4\",\n",
    "    \"fiJxCVA\",\n",
    "    \"pCAIlxD\",\n",
    "    \"zJx2skh\",\n",
    "    \"2Gdl1u7\",\n",
    "    \"aJJAY4c\",\n",
    "    \"ros6RLC\",\n",
    "    \"DKLBJh7\",\n",
    "    \"eyxH0Wc\",\n",
    "    \"rJEkEw4\"]\n",
    "    return HTML(\"\"\"\n",
    "    <video alt=\"test\" controls autoplay=1>\n",
    "        <source src=\"https://openpuppies.com/mp4/%s.mp4\"  type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\"%(random.sample(pups, 1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceeca999-cab3-4351-81a9-a955c8b2c62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: triton\n",
      "Version: 2.3.0\n",
      "Summary: A language and compiler for custom Deep Learning operations\n",
      "Home-page: https://github.com/openai/triton/\n",
      "Author: Philippe Tillet\n",
      "Author-email: phil@openai.com\n",
      "License: \n",
      "Location: /usr/lib64/python3.12/site-packages\n",
      "Requires: filelock\n",
      "Required-by: triton-viz\n"
     ]
    }
   ],
   "source": [
    "!pip show triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8e9489-6c58-4ff3-97f1-b61288bd748b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/lib64/python3.12/site-packages/triton/__init__.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import triton\n",
    "triton.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f4e671-8ca5-4a23-ac76-ecc9a8101045",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'InterpretedFunction' from 'triton.interpreter.interpreter' (/usr/lib64/python3.12/site-packages/triton/interpreter/interpreter.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interpreter\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtriton\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InterpretedFunction\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'InterpretedFunction' from 'triton.interpreter.interpreter' (/usr/lib64/python3.12/site-packages/triton/interpreter/interpreter.py)"
     ]
    }
   ],
   "source": [
    "from triton import interpreter\n",
    "from triton.interpreter.interpreter import InterpretedFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c058e98-942b-43d1-b85a-8eb6d6ebbd1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AutotuneGridSelector',\n",
       " 'AutotuneRunner',\n",
       " 'DebuggerFunction',\n",
       " 'ExecutionContext',\n",
       " 'GridSelector',\n",
       " 'MemoryMap',\n",
       " 'TritonLangProxy',\n",
       " 'Tuple',\n",
       " 'WrappedTensor',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_primitive_to_tensor',\n",
       " 'attach_triton',\n",
       " 'debugger_constexpr',\n",
       " 'detach_triton',\n",
       " 'get_proxy_method',\n",
       " 'itertools',\n",
       " 'lcore',\n",
       " 'program_ids_from_grid',\n",
       " 'random',\n",
       " 'tl',\n",
       " 'tl_method_backup',\n",
       " 'torch',\n",
       " 'torch_wrapper',\n",
       " 'triton_debug_autotune']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(triton.interpreter.interpreter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c39bfd-04dd-44d6-96fe-7a05a87ec980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: triton\n",
      "Version: 2.3.0\n",
      "Summary: A language and compiler for custom Deep Learning operations\n",
      "Home-page: https://github.com/openai/triton/\n",
      "Author: Philippe Tillet\n",
      "Author-email: phil@openai.com\n",
      "License: \n",
      "Location: /usr/lib64/python3.12/site-packages\n",
      "Requires: filelock\n",
      "Required-by: triton-viz\n"
     ]
    }
   ],
   "source": [
    "!pip show triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "108aab1f-5657-4c9b-ba71-034eb0dbf315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autotuner\n",
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n",
      "cache\n",
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n",
      "driver\n",
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n",
      "errors\n",
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n",
      "jit\n",
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n"
     ]
    }
   ],
   "source": [
    "import pkgutil\n",
    "\n",
    "def list_modules(namespace):\n",
    "    \"\"\"List all modules and submodules in a given namespace.\"\"\"\n",
    "    for importer, modname, ispkg in pkgutil.iter_modules(namespace.__path__):\n",
    "        print(modname)\n",
    "        print(dir(modname))\n",
    "        if ispkg:\n",
    "            subpkg = namespace.__name__ + '.' + modname\n",
    "            print(f\"Submodules of {subpkg}:\")\n",
    "            list_modules(__import__(subpkg))\n",
    "\n",
    "# Example usage:\n",
    "list_modules(triton.runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e555eb65-cfab-4268-a2e8-5a5dbf9a554d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Autotuner',\n",
       " 'Config',\n",
       " 'Heuristics',\n",
       " 'JITFunction',\n",
       " 'KernelInterface',\n",
       " 'MockTensor',\n",
       " 'OutOfResources',\n",
       " 'TensorWrapper',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'autotune',\n",
       " 'autotuner',\n",
       " 'cache',\n",
       " 'driver',\n",
       " 'heuristics',\n",
       " 'jit',\n",
       " 'reinterpret',\n",
       " 'version_key']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(triton.runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904191a9-6ed6-4759-9ace-5a901db9f8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax(Y, stride_ym, stride_yn, X, stride_xm, stride_xn, M, N):\n",
    "    # row index\n",
    "    m = tl.program_id(0)\n",
    "    # col indices\n",
    "    # this specific kernel only works for matrices that \n",
    "    # have less than BLOCK_SIZE columns\n",
    "    BLOCK_SIZE: tl.constexpr = 1024\n",
    "    n = tl.arange(0, BLOCK_SIZE)\n",
    "    # the memory address of all the elements\n",
    "    # that we want to load can be computed as follows\n",
    "    X = X + m * stride_xm + n * stride_xn\n",
    "    # load input data; pad out-of-bounds elements with 0 \n",
    "    x = tl.load(X, mask=n < N, other=-float('inf'))\n",
    "    # compute numerically-stable softmax\n",
    "    z = x - tl.max(x, axis=0)\n",
    "    num = tl.exp(z)\n",
    "    denom = tl.sum(num, axis=0)\n",
    "    y = num / denom\n",
    "    # write back to Y\n",
    "    Y = Y + m * stride_ym + n * stride_yn\n",
    "    tl.store(Y, y, mask=n < N)\n",
    "\n",
    "import torch\n",
    "# Allocate input/output tensors\n",
    "X = torch.normal(0, 1, size=(512, 512), device='cuda')\n",
    "Y = torch.empty_like(X, device='cuda')\n",
    "# SPMD launch grid\n",
    "grid = (X.shape[0], )\n",
    "# enqueue GPU kernel\n",
    "softmax[grid](Y, Y.stride(0), Y.stride(1), \n",
    "              X, X.stride(0), X.stride(1),\n",
    "              X.shape[0]    , X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9deb0127-1694-41b6-abb0-4ad27d9309f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.0.32831-'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.hip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37eaf287-8690-4a41-925b-3f86cb6b2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton.runtime as tr\n",
    "import triton.runtime.driver as trd\n",
    "import triton.runtime.jit as trjit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74b974cb-32a2-4ee3-9eaa-783f382d2636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trjit.get_device_capability(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1809844b-f637-495d-8bb4-1d2459817ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Callable',\n",
       " 'DependenciesFinder',\n",
       " 'Generic',\n",
       " 'Iterable',\n",
       " 'JITFunction',\n",
       " 'KernelInterface',\n",
       " 'List',\n",
       " 'MockTensor',\n",
       " 'Optional',\n",
       " 'T',\n",
       " 'TMAInfos',\n",
       " 'TRITON_PATH',\n",
       " 'TRITON_VERSION',\n",
       " 'TensorWrapper',\n",
       " 'TypeVar',\n",
       " 'Union',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_normalize_ty',\n",
       " 'annotations',\n",
       " 'ast',\n",
       " 'cast',\n",
       " 'defaultdict',\n",
       " 'division',\n",
       " 'dtype',\n",
       " 'functools',\n",
       " 'get_backend',\n",
       " 'get_cuda_stream',\n",
       " 'get_current_device',\n",
       " 'get_device_capability',\n",
       " 'hashlib',\n",
       " 'inspect',\n",
       " 'jit',\n",
       " 'namedtuple',\n",
       " 'os',\n",
       " 'overload',\n",
       " 'path_to_ptxas',\n",
       " 'reinterpret',\n",
       " 'set_current_device',\n",
       " 'subprocess',\n",
       " 'textwrap',\n",
       " 'version_key']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(trjit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f67ea99-c112-43f6-838e-1597d693d189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: triton\n",
      "Version: 2.3.0\n",
      "Summary: A language and compiler for custom Deep Learning operations\n",
      "Home-page: https://github.com/openai/triton/\n",
      "Author: Philippe Tillet\n",
      "Author-email: phil@openai.com\n",
      "License: \n",
      "Location: /usr/lib64/python3.12/site-packages\n",
      "Requires: filelock\n",
      "Required-by: triton-viz\n"
     ]
    }
   ],
   "source": [
    "!pip show triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2610954c-8a4e-46c1-8285-2bb8b94a9038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: triton-viz\n",
      "Version: 0.1\n",
      "Summary: A visualization tool for Triton\n",
      "Home-page: https://github.com/Deep-Learning-Profiling-Tools/triton-viz\n",
      "Author: Deep Learning Profiling Tools Team\n",
      "Author-email: kzhou6@gmu.edu\n",
      "License: \n",
      "Location: /home/kge/.local/lib/python3.12/site-packages\n",
      "Editable project location: /home/kge/triton_viz_install/triton-viz\n",
      "Requires: chalk-diagrams, gradio, pre-commit, pyarrow, pytest, setuptools, triton\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show triton_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0703b26f-2452-42c0-b50a-3b85132d2a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib pandas -q\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0/ -q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
