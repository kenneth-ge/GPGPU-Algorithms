{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564fd290-4e55-4119-88a4-0f77536992c5",
   "metadata": {},
   "source": [
    "# Essentially prefix sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc7777ee-11ec-47b6-aa73-05e848650525",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib64/python3.12/site-packages/numba-0+unknown-py3.12-linux-x86_64.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/f3/82/68ccd49add4d21937f087871350905ffc709f32c92bf95334e7abf442147/torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting triton\n",
      "  Obtaining dependency information for triton from https://files.pythonhosted.org/packages/ea/a4/e66cbd7befaf44a84cfb367b00a0331735cd56d4b2076533dec9b0b255fe/triton-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading triton-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting jaxtyping\n",
      "  Obtaining dependency information for jaxtyping from https://files.pythonhosted.org/packages/2d/4a/8949827db6ddad0cd773ce14603d86fc4c0893598417cf00c3cdc2192bc5/jaxtyping-0.2.30-py3-none-any.whl.metadata\n",
      "  Downloading jaxtyping-0.2.30-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3.12/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/lib/python3.12/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3.12/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-nvrtc-cu12==12.1.105 from https://files.pythonhosted.org/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-runtime-cu12==12.1.105 from https://files.pythonhosted.org/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-cupti-cu12==12.1.105 from https://files.pythonhosted.org/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Obtaining dependency information for nvidia-cublas-cu12==12.1.3.1 from https://files.pythonhosted.org/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Obtaining dependency information for nvidia-cufft-cu12==11.0.2.54 from https://files.pythonhosted.org/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Obtaining dependency information for nvidia-curand-cu12==10.3.2.106 from https://files.pythonhosted.org/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusolver-cu12==11.4.5.107 from https://files.pythonhosted.org/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusparse-cu12==12.1.0.106 from https://files.pythonhosted.org/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Obtaining dependency information for nvidia-nccl-cu12==2.20.5 from https://files.pythonhosted.org/packages/4b/2a/0a131f572aa09f741c30ccd45a8e56316e8be8dfc7bc19bf0ab7cfef7b19/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-nvtx-cu12==12.1.105 from https://files.pythonhosted.org/packages/da/d3/8057f0587683ed2fcd4dbfbdfdfa807b9160b809976099d36b8f60d08f03/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/16/03/7e96a2ccbb752857f50c0c1355b1c52d5922be43fe0691847e520750e5c7/nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting typeguard==2.13.3 (from jaxtyping)\n",
      "  Obtaining dependency information for typeguard==2.13.3 from https://files.pythonhosted.org/packages/9a/bb/d43e5c75054e53efce310e79d63df0ac3f25e34c926be5dffb7d283fb2a8/typeguard-2.13.3-py3-none-any.whl.metadata\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib64/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.3.1-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jaxtyping-0.2.30-py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typeguard, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, nvidia-cusolver-cu12, torch\n",
      "Successfully installed jaxtyping-0.2.30 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torch-2.3.1 triton-2.3.1 typeguard-2.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch triton jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb96944-6fe7-4c90-be89-e44a527a91d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "from torch import Tensor\n",
    "import triton.language as tl\n",
    "import jaxtyping\n",
    "from jaxtyping import Float32, Int32\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec8c3276-19f7-44e7-9a20-41595a980ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(8, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9387eefc-5215-47e0-bfbb-d4d01dbb097a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a56b11-fe30-4072-bb53-7e22653e5de5",
   "metadata": {},
   "source": [
    "# CPU version of parallel prefix sum/scan\n",
    "this is the \"work-inefficient\" version, but technically it has span O(log n) and work O(n) at each iteration, so it's not actually that bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9ec5500-b543-40dd-b0b8-01ceb7ecf384",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.clone(X) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00411d5c-6a92-45e2-a47a-1d3a74cfcd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7, 8], device='cuda:0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b35c3c2c-f148-470b-8300-fe0f64f5dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  3,  5,  7,  9, 11, 13, 15], device='cuda:0')\n",
      "tensor([ 1,  3,  6, 10, 14, 18, 22, 26], device='cuda:0')\n",
      "tensor([ 1,  3,  6, 10, 15, 21, 28, 36], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# assume n is a power of 2\n",
    "n = 8\n",
    "stride = 1\n",
    "for i in range(0, int(math.log2(n))):\n",
    "    for j in range(n - 1, stride - 1, -1):\n",
    "        Y[j] += Y[j - stride]\n",
    "    stride *= 2\n",
    "    print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f64751bf-1da8-49e7-9d48-43ca4de52c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  6, 10, 15, 21, 28, 36], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45f1bd3e-e4cc-4071-b4ab-7ca76f42ca87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  6, 10, 15, 21, 28, 36], device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(torch.clone(X) + 1, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c888b2a-224f-4a9f-b9aa-1a3572fe4c72",
   "metadata": {},
   "source": [
    "# Triton version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "194e4c1b-ae5e-4bfa-85d9-5be962776e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def scan(Y, nextY, stride, BLOCK_SIZE: tl.constexpr):\n",
    "    pid_row = tl.program_id(0)\n",
    "\n",
    "    for j in tl.static_range(BLOCK_SIZE):\n",
    "        current_idx = pid_row * BLOCK_SIZE + j\n",
    "        if current_idx - stride >= 0:\n",
    "            Yj = tl.load(Y + current_idx)\n",
    "            Yjminstride = tl.load(Y + current_idx - stride)\n",
    "            \n",
    "            tl.store(nextY + current_idx, Yj + Yjminstride)\n",
    "        else:\n",
    "            tl.store(nextY + current_idx, tl.load(Y + current_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "560fba42-e116-4fe8-a205-7ddb3cd920b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  3,  6, 10, 15, 21, 28], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def triton_pref_sum(X, BLOCK_SIZE):\n",
    "    Y = torch.clone(X)\n",
    "    Ynext = torch.empty_like(Y)\n",
    "    n = X.shape[0]\n",
    "    stride = 1\n",
    "    for i in range(0, int(math.log2(n))):\n",
    "        scan[(math.ceil(n / BLOCK_SIZE),)](Y, Ynext, stride, BLOCK_SIZE)\n",
    "        stride *= 2\n",
    "        Ynext, Y = Y, Ynext\n",
    "\n",
    "    return Y\n",
    "triton_pref_sum(X, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f88824be-2ba4-4b5a-b108-cf7bcf83c5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  3,  6, 10, 15, 21, 28], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cumsum(X, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d17a95-dd79-491d-bb04-77d51e13b8d0",
   "metadata": {},
   "source": [
    "# OpenCL version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f8647a4a-dafa-407a-bc70-c23c28c77f2d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib64/python3.12/site-packages/numba-0+unknown-py3.12-linux-x86_64.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pyopencl\n",
      "  Obtaining dependency information for pyopencl from https://files.pythonhosted.org/packages/aa/f9/a93082383350d818378072b390c8d1288209a000739454d04226b05048c0/pyopencl-2024.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading pyopencl-2024.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: numpy in /usr/lib64/python3.12/site-packages (from pyopencl) (1.24.4)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /usr/lib/python3.12/site-packages (from pyopencl) (3.9.1)\n",
      "Collecting pytools>=2024.1.5 (from pyopencl)\n",
      "  Obtaining dependency information for pytools>=2024.1.5 from https://files.pythonhosted.org/packages/73/61/831688725435cfdc405bf6d413082e1cc8a3287a6d88f67bf77ead20bcfe/pytools-2024.1.5-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytools-2024.1.5-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading pyopencl-2024.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (697 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m697.7/697.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytools-2024.1.5-py2.py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytools, pyopencl\n",
      "Successfully installed pyopencl-2024.2.6 pytools-2024.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pyopencl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1b34bc-b71e-4b00-b2ae-6283f50caae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pyopencl as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dab4c736-4e42-475e-b192-e417dd8baa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up OpenCL context\n",
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e3ea1e1-4bfb-4e91-a387-2631a190c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into OpenCL\n",
    "mf = cl.mem_flags\n",
    "# a_g = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=a_np)\n",
    "# | mf.COPY_HOST_PTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "40db55a5-cc26-49a4-9c43-46e47a11672b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kge'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e7339f34-c7cd-4ab6-8364-25a909c1eb34",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ALLOC_HOST_PTR',\n",
       " 'COPY_HOST_PTR',\n",
       " 'HOST_NO_ACCESS',\n",
       " 'HOST_READ_ONLY',\n",
       " 'HOST_WRITE_ONLY',\n",
       " 'KERNEL_READ_AND_WRITE',\n",
       " 'READ_ONLY',\n",
       " 'READ_WRITE',\n",
       " 'USE_HOST_PTR',\n",
       " 'WRITE_ONLY',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_is_bitfield',\n",
       " 'to_string']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ff6b59d-8b1f-4e61-a062-ffe3c398d497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYOPENCL_COMPILER_OUTPUT=1\n"
     ]
    }
   ],
   "source": [
    "%env PYOPENCL_COMPILER_OUTPUT=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01fa795b-521d-4249-adda-985e5c926d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create kernel\n",
    "# open('./CUDA Puzzles/prefixsum.comp', 'r').read()\n",
    "prg = cl.Program(ctx, open('./kernels.comp', 'r').read()).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1849a3-e9cb-4788-9f06-2d87537c0a48",
   "metadata": {},
   "source": [
    "### Test fill with wrong datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e827529-f53a-4df0-b9b6-94e7e3159cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "[4575657222473777152 4575657222473777152 4575657222473777152\n",
      " 4575657222473777152                   0                   0\n",
      "                   0                   0]\n",
      "[0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pyopencl as cl\n",
    "\n",
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx)\n",
    "\n",
    "mf = cl.mem_flags\n",
    "\n",
    "prg = cl.Program(ctx, \"\"\"__kernel void fill(__global float *res_g)\n",
    "{\n",
    "    int gid = get_global_id(0);\n",
    "\n",
    "    res_g[gid] = 1.0;\n",
    "}\"\"\").build()\n",
    "\n",
    "# create numpy starting array\n",
    "a_np = np.arange(8)\n",
    "a_g = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=a_np)\n",
    "res_g = cl.Buffer(ctx, mf.READ_WRITE, a_np.nbytes)\n",
    "res_np = np.empty_like(a_np)\n",
    "\n",
    "prg.fill(queue, res_np.shape, None, res_g)\n",
    "\n",
    "cl.enqueue_copy(queue, res_np, res_g)\n",
    "\n",
    "print(res_np)\n",
    "print(a_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20024a96-7466-47dd-92b2-7a0b7dfa56e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(8).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1ebab-3d24-4c79-a29f-0bab7a1f745c",
   "metadata": {},
   "source": [
    "### Test fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68df69ed-da0d-401b-b582-3e92cc0d8cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[0. 1. 2. 3. 4. 5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "# create numpy starting array\n",
    "a_np = np.arange(8).astype(np.float32)\n",
    "a_g = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=a_np)\n",
    "res_g = cl.Buffer(ctx, mf.READ_WRITE, a_np.nbytes)\n",
    "res_np = np.empty_like(a_np)\n",
    "\n",
    "print(a_np.shape)\n",
    "prg.fill(queue, res_np.shape, None, res_g)\n",
    "\n",
    "cl.enqueue_copy(queue, res_np, res_g)\n",
    "\n",
    "print(res_np)\n",
    "print(a_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee819e6e-b4b5-40e2-99af-21e06746f79d",
   "metadata": {},
   "source": [
    "### Test copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5208d103-4486-4318-bf83-1da7ec6cfb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "[0. 1. 2. 3. 4. 5. 6. 7.]\n",
      "[0. 1. 2. 3. 4. 5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "# create numpy starting array\n",
    "a_np = np.arange(8).astype(np.float32)\n",
    "a_g = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=a_np)\n",
    "res_g = cl.Buffer(ctx, mf.READ_WRITE, a_np.nbytes)\n",
    "res_np = np.empty_like(a_np)\n",
    "\n",
    "print(a_np.shape)\n",
    "prg.copy(queue, res_np.shape, None, a_g, res_g)\n",
    "\n",
    "cl.enqueue_copy(queue, res_np, res_g)\n",
    "\n",
    "print(res_np)\n",
    "print(a_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db681f4e-0f2e-4946-91f0-e7f9628c9d13",
   "metadata": {},
   "source": [
    "### Test scan step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca6229bc-2891-4175-8a4a-bae3d89be54c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "[ 0.  1.  3.  5.  7.  9. 11. 13.]\n",
      "[0. 1. 2. 3. 4. 5. 6. 7.]\n"
     ]
    }
   ],
   "source": [
    "# create numpy starting array\n",
    "a_np = np.arange(8).astype(np.float32)\n",
    "a_g = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=a_np)\n",
    "res_g = cl.Buffer(ctx, mf.READ_WRITE, a_np.nbytes)\n",
    "res_np = np.empty_like(a_np)\n",
    "\n",
    "print(a_np.shape)\n",
    "prg.psum_step(queue, res_np.shape, None, a_g, res_g, np.int32(1))\n",
    "\n",
    "cl.enqueue_copy(queue, res_np, res_g)\n",
    "\n",
    "print(res_np)\n",
    "print(a_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a75c6294-bc30-4a78-9a23-434d0046d18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kge/.local/lib/python3.12/site-packages/pyopencl/__init__.py:519: CompilerWarning: From-source build succeeded, but resulted in non-empty logs:\n",
      "Build on <pyopencl.Device 'NVIDIA A100-PCIE-40GB' on 'NVIDIA CUDA' at 0x560bae081ec0> succeeded, but said:\n",
      "\n",
      "(): Warning: Function fill is a kernel, so overriding noinline attribute. The function may be inlined when called.\n",
      "(): Warning: Function copy is a kernel, so overriding noinline attribute. The function may be inlined when called.\n",
      "(): Warning: Function psum_step is a kernel, so overriding noinline attribute. The function may be inlined when called.\n",
      "\n",
      "\n",
      "  lambda: self._prg.build(options_bytes, devices),\n"
     ]
    }
   ],
   "source": [
    "prg = cl.Program(ctx, open('./kernels.comp', 'r').read()).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36b63d85-19c2-4b8e-8864-a6a3ca2c0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# create result buffer, queue kernel, and copy result to host (CPU)\n",
    "def opencl_prefix_sum(input_arr_np):\n",
    "    a_np = input_arr_np.astype(np.float32)\n",
    "    tmp_viewing_buff = np.empty_like(a_np)\n",
    "    a_g = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=a_np)\n",
    "    res_g = cl.Buffer(ctx, mf.READ_WRITE, a_np.nbytes)\n",
    "    knl = prg.psum_step\n",
    "    \n",
    "    n = a_np.shape[0]\n",
    "    stride = np.int32(1)\n",
    "    for i in range(0, ceil(math.log2(n))):\n",
    "        knl(queue, a_np.shape, None, a_g, res_g, stride)\n",
    "        stride *= 2\n",
    "        stride = np.int32(stride)\n",
    "        \n",
    "        cl.enqueue_copy(queue, tmp_viewing_buff, a_g)\n",
    "        print('a', tmp_viewing_buff)\n",
    "        cl.enqueue_copy(queue, tmp_viewing_buff, res_g)\n",
    "        print('res', tmp_viewing_buff)\n",
    "        a_g, res_g = res_g, a_g\n",
    "    \n",
    "    res_np = np.empty_like(a_np)\n",
    "    cl.enqueue_copy(queue, res_np, a_g)\n",
    "    print('final res:', res_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a9f93067-177b-452e-aa07-de6264d26e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [1. 2. 3. 4. 5. 6. 7. 8.]\n",
      "res [ 1.  3.  5.  7.  9. 11. 13. 15.]\n",
      "a [ 1.  3.  5.  7.  9. 11. 13. 15.]\n",
      "res [ 1.  3.  6. 10. 14. 18. 22. 26.]\n",
      "a [ 1.  3.  6. 10. 14. 18. 22. 26.]\n",
      "res [ 1.  3.  6. 10. 15. 21. 28. 36.]\n",
      "final res: [ 1.  3.  6. 10. 15. 21. 28. 36.]\n"
     ]
    }
   ],
   "source": [
    "opencl_prefix_sum(np.arange(8) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cfebf398-a751-4205-a710-1cfd34540a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  3,  6, 10, 15, 21, 28, 36])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(np.arange(8) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b886c47-61bb-4e03-a389-b733b61d34b6",
   "metadata": {},
   "source": [
    "# Numba version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3cd39323-26f4-4001-abfd-f07990e32cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def numba_psum(input_array, output_array, stride, N):\n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    ty = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    # Compute flattened index inside the array\n",
    "    pos = tx + ty * bw\n",
    "\n",
    "    back_pos = pos - stride\n",
    "\n",
    "    if pos >= N:\n",
    "        return\n",
    "    \n",
    "    if back_pos < 0:\n",
    "        output_array[pos] = input_array[pos]\n",
    "    else:\n",
    "        output_array[pos] = input_array[pos] + input_array[back_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1e28d7a-8b22-44cb-8f6f-3e64f893a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ary = np.arange(10)\n",
    "d_ary = cuda.to_device(ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "474ab3a8-c74b-4885-a4ba-6e807318afe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hary = d_ary.copy_to_host()\n",
    "hary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42d975bc-db52-4e84-bdcc-c3f65214fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current = cuda.to_device(np.arange(10))\n",
    "next = cuda.device_array(current.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ace2c781-c34d-4099-a439-86c898498856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.12/site-packages/numba-0+unknown-py3.12-linux-x86_64.egg/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "numba_psum[1, 10](current, next, 1, current.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97146602-d843-43ae-b0a9-0e618252cfb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  3.,  5.,  7.,  9., 11., 13., 15., 17.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_next = next.copy_to_host()\n",
    "h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d65ae67-8a90-4c2c-a44d-6dcb7fa7a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# create result buffer, queue kernel, and copy result to host (CPU)\n",
    "def numba_prefix_sum(inp):\n",
    "    inp_cuda = cuda.to_device(inp)\n",
    "    next_cuda = cuda.device_array(inp_cuda.shape)\n",
    "    \n",
    "    n = inp.shape[0]\n",
    "    stride = 1\n",
    "    for i in range(0, math.ceil(math.log2(n))):\n",
    "        numba_psum[math.ceil(n / 1024), 1024](inp_cuda, next_cuda, stride, n)\n",
    "        stride *= 2\n",
    "\n",
    "        inp_cuda, next_cuda = next_cuda, inp_cuda\n",
    "\n",
    "    return inp_cuda.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "270115aa-4c8c-47c3-aa73-c3766d214740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   3,   6,  10,  15,  21,  28,  36,  45,  55,  66,  78,\n",
       "        91, 105])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numba_prefix_sum(np.arange(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb6625a3-5e4b-4e0c-839a-2b245e7a00b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   3,   6,  10,  15,  21,  28,  36,  45,  55,  66,  78,\n",
       "        91, 105])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(np.arange(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1da2c8-8752-4487-a9c5-1294f0d2c568",
   "metadata": {},
   "source": [
    "# CUDA version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce188d-6fec-403a-9e0b-653a8f3fbf39",
   "metadata": {},
   "source": [
    "CUDA's recommended Python container is Numba. So, we will swap down to CUDA C++ for this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "deca6e44-a6e0-4e44-923a-0935473caac1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib64/python3.12/site-packages/numba-0+unknown-py3.12-linux-x86_64.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
      "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-8a27c3in\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-8a27c3in\n",
      "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 801584cceb559adc54e828ebe9b385c5f53fe70f\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: nvcc4jupyter\n",
      "  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10743 sha256=3a570d3ab74e9051a615aa78d54722c78a69a0c83a1d407fccde14c37a30e96c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-34e9bp5d/wheels/7d/b9/66/459b9938664e6a93d1a85323ec52f7e51cd7265d253410a7d8\n",
      "Successfully built nvcc4jupyter\n",
      "Installing collected packages: nvcc4jupyter\n",
      "Successfully installed nvcc4jupyter-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79c36d7-f40d-4128-887e-4211f7918aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files will be saved in \"/tmp/tmpqfjj2snt\".\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc4jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bef1a896-36b0-42c3-8c83-9b8161ff94f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <iostream>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "int main() {\n",
    "    std::cout << \"Hello!\" << endl;\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbb206f8-12f1-4486-ac1a-b599a97d2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 3 6 10 15 21 28 36 45 55 66 78 91 105 120 136 153 171 190 210 231 253 276 300 325 351 378 406 435 465 496 528 561 595 630 666 703 741 780 820 861 903 946 990 1035 1081 1128 1176 1225 1275 1326 1378 1431 1485 1540 1596 1653 1711 1770 1830 1891 1953 2016 2080 2145 2211 2278 2346 2415 2485 2556 2628 2701 2775 2850 2926 3003 3081 3160 3240 3321 3403 3486 3570 3655 3741 3828 3916 4005 4095 4186 4278 4371 4465 4560 4656 4753 4851 4950 \n",
      "Max error is: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <iostream>\n",
    "#include<cstdlib>\n",
    "#include <utility>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "__global__ void psum(int n, int stride, float* curr, float* next) {\n",
    "  int i = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "  if (i < n){\n",
    "    if(i - stride >= 0)\n",
    "        next[i] = curr[i] + curr[i - stride];\n",
    "    else\n",
    "        next[i] = curr[i];\n",
    "  }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int N = 100;\n",
    "\n",
    "    float* arr = new float[N];\n",
    "\n",
    "    srand((unsigned) time(NULL));\n",
    "\n",
    "    for(int i = 0; i < N; i++){\n",
    "        arr[i] = i;//rand() % 10;\n",
    "    }\n",
    "\n",
    "    float *dev_curr, *dev_next;\n",
    "\n",
    "    cudaMalloc(&dev_curr, N*sizeof(float));\n",
    "    cudaMalloc(&dev_next, N*sizeof(float));\n",
    "\n",
    "    cudaMemcpy(dev_curr, arr, N*sizeof(float), cudaMemcpyHostToDevice);\n",
    "\n",
    "    // (N + 255) / 256 is equivalent to ceil(N / 256)\n",
    "    int stride = 1;\n",
    "\n",
    "    while(stride <= N){\n",
    "        psum<<<(N + 255) / 256, 256>>>(N, stride, dev_curr, dev_next);\n",
    "        stride *= 2;\n",
    "        swap(dev_curr, dev_next);\n",
    "    }\n",
    "\n",
    "    float* final_ans = new float[N];\n",
    "    cudaMemcpy(final_ans, dev_curr, N*sizeof(float), cudaMemcpyDeviceToHost);\n",
    "\n",
    "    float* actual_ans = new float[N];\n",
    "\n",
    "    actual_ans[0] = arr[0];\n",
    "    for(int i = 1; i < N; i++){\n",
    "        actual_ans[i] = actual_ans[i - 1] + arr[i];\n",
    "    }\n",
    "\n",
    "    float maxErr = 0.0f;\n",
    "\n",
    "    for(int i = 0; i < N; i++){\n",
    "        cout << final_ans[i] << \" \";\n",
    "        maxErr = max(maxErr, abs(final_ans[i] - actual_ans[i]));\n",
    "    }\n",
    "\n",
    "    cout << endl << \"Max error is: \" << maxErr << endl;\n",
    "\n",
    "    delete [] actual_ans;\n",
    "    delete [] final_ans;\n",
    "    delete [] arr;\n",
    "\n",
    "    cudaFree(dev_curr);\n",
    "    cudaFree(dev_next);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e56509-4913-45e6-8c7b-d6bbe6aeb021",
   "metadata": {},
   "source": [
    "# HIP version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3314700-ca1c-465f-b118-6ca29d591fa8",
   "metadata": {},
   "source": [
    "Since this is an Nvidia machine, I am simply pasting the HIP code here. Unfortunately, due to kernel and driver errors, this didn't work. But, I trust that the algorithm and code should be correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045826fa-c5c9-4246-a8a8-878ff0af0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <time.h>\n",
    "#include <utility>\n",
    "#include <algorithm>\n",
    "#include \"hip/hip_runtime.h\"\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "template <typename T>\n",
    "__global__ void\n",
    "psum_step(T *curr, T *next, size_t N, size_t stride)\n",
    "{\n",
    "    size_t pos = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if(pos < N){\n",
    "        next[pos] = curr[pos];\n",
    "        if(pos - stride >= 0){\n",
    "            next[pos] += curr[pos - stride];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main(){\n",
    "    int N = 100;\n",
    "    \n",
    "    float* arr = new float[N];\n",
    "\n",
    "    srand(time(NULL));\n",
    "    for(int i = 0; i < N; i++){\n",
    "        arr[i] = rand() % 10;\n",
    "    }\n",
    "\n",
    "    float* psumAns = new float[N];\n",
    "\n",
    "    psumAns[0] = arr[0];\n",
    "    for(int i = 1; i < N; i++){\n",
    "        psumAns[i] = psumAns[i - 1] + arr[i];\n",
    "    }\n",
    "\n",
    "    float *curr, *next;\n",
    "\n",
    "    hipMalloc(&curr, sizeof(float) * N);\n",
    "    hipMalloc(&next, sizeof(float) * N);\n",
    "\n",
    "    hipMemcpy(curr, arr, sizeof(float) * N, hipMemcpyHostToDevice);\n",
    "\n",
    "    cout << \"got here\" << endl;\n",
    "    \n",
    "    int stride = 1;\n",
    "    while(stride < N){\n",
    "        hipLaunchKernelGGL(psum_step, \n",
    "            dim3((N + 255) / 256), dim3(256), 0/*dynamic shared*/, 0/*stream*/,     /* launch config*/\n",
    "            curr, next, N, stride);\n",
    "\n",
    "        cout << \"launched kernel\" << endl;\n",
    "        \n",
    "        swap(curr, next);\n",
    "        stride *= 2;\n",
    "    }\n",
    "    \n",
    "    float* hipRet = new float[N];\n",
    "\n",
    "    cout << \"Created hipRet\" << endl;\n",
    "    \n",
    "    hipMemcpy(hipRet, curr, sizeof(float) * N, hipMemcpyDeviceToHost);\n",
    "\n",
    "    cout << \"memcpy\" << endl;\n",
    "    \n",
    "    cout << \"Arr: \";\n",
    "    for(int i = 0; i < N; i++)\n",
    "        cout << arr[i] << \" \";\n",
    "    cout << endl;\n",
    "\n",
    "    cout << \"HIP: \";\n",
    "    for(int i = 0; i < N; i++)\n",
    "        cout << hipRet[i] << \" \";\n",
    "    cout << endl;\n",
    "\n",
    "    cout << \"Actual: \";\n",
    "    for(int i = 0; i < N; i++)\n",
    "        cout << psumAns[i] << \" \";\n",
    "    cout << endl;\n",
    "\n",
    "    float maxDiff = 0;\n",
    "    for(int i = 0; i < N; i++)\n",
    "        maxDiff = max(maxDiff, abs(psumAns[i] - hipRet[i]));\n",
    "    cout << \"Max diff: \" << maxDiff << endl;\n",
    "    \n",
    "    hipFree(&curr);\n",
    "    hipFree(&next);\n",
    "\n",
    "    delete [] hipRet;\n",
    "    delete [] arr;\n",
    "    delete [] psumAns;\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
