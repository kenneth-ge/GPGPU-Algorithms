{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19204358-a9ba-429f-9275-5a33bf607918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91af2530-c387-444e-90ff-2e3339000da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "#tokenizer.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dc6e2cd-9b6e-4427-8747-8eec80ae8515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    # Tokenize prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "    \n",
    "    # Generate output\n",
    "    output = model.generate(input_ids)#, max_length=200, num_return_sequences=1, temperature=0.9)\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fffce91-2864-4e3d-9fe5-2cfe0a09d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family means nobody gets left behind, or forgotten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kge/.local/lib/python3.12/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(generate(\n",
    "    \"\"\"Read out the following LaTeX command. Read it as if I'm trying to generate a natural language equivalent for this command, so that it can be given into a document authoring tool and result in this command being the output. Do not elaborate or try to predict what comes next. Give me only the readaloud output, do not explain: \\n\n",
    "Family means nobody gets left behind, or forgotten. \\\\\\\\ \\\\medskip\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "174ed06a-88e9-453b-a520-8b740a5b2924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n",
      "Family means nobody gets left behind, or forgotten.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Read out the following LaTeX command. Read it as if I'm trying to generate a natural language equivalent for this command, so that it can be given into a document authoring tool and result in this command being the output. Do not elaborate or try to predict what comes next. Give me only the readaloud output, do not explain: \\n\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43;03mFamily means nobody gets left behind, or forgotten. \\\\\\\\ \\\\medskip\"\"\"\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate output\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#, max_length=200, num_return_sequences=1, temperature=0.9)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Decode output\u001b[39;00m\n\u001b[1;32m      9\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:1758\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1751\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1752\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1753\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1755\u001b[0m     )\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1758\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1772\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/generation/utils.py:2397\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2394\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2396\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2397\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2398\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2400\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2401\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2402\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1740\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1737\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1740\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1107\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1093\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1094\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         output_attentions,\n\u001b[1;32m   1105\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1107\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:687\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 687\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    697\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:594\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    585\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    592\u001b[0m ):\n\u001b[1;32m    593\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 594\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    604\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:566\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    563\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m*\u001b[39m layer_head_mask\n\u001b[1;32m    565\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m unshape(torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states))  \u001b[38;5;66;03m# (batch_size, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m present_key_value_state \u001b[38;5;241m=\u001b[39m (key_states, value_states) \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m use_cache) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (attn_output,) \u001b[38;5;241m+\u001b[39m (present_key_value_state,) \u001b[38;5;241m+\u001b[39m (position_bias,)\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib64/rocm/gfx9/lib64/python3.12/site-packages/torch/nn/modules/module.py:1534\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1532\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1535\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    print(generate(\n",
    "    \"\"\"Read out the following LaTeX command. Read it as if I'm trying to generate a natural language equivalent for this command, so that it can be given into a document authoring tool and result in this command being the output. Do not elaborate or try to predict what comes next. Give me only the readaloud output, do not explain: \\n\n",
    "Family means nobody gets left behind, or forgotten. \\\\\\\\ \\\\medskip\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eef9ab-120e-403a-837e-ffaebc046fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
